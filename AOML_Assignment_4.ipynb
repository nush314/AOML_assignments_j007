{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH24Ylnpw8LP",
        "outputId": "700090c9-9d7e-41d6-bc72-f5f0af253094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Optimized Train RMSE: 2.0935\n",
            "ğŸš€ Optimized Validation RMSE: 3.5367\n",
            "âœ… Submission file saved as 'submission.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv(\"/content/train (1).csv\")\n",
        "test = pd.read_csv(\"/content/test (1).csv\")\n",
        "\n",
        "# Identify target variable\n",
        "target_column = \"output_electricity_generation\"\n",
        "\n",
        "# Ensure target column exists in train but NOT in test\n",
        "if target_column not in train.columns:\n",
        "    raise ValueError(f\"Column '{target_column}' is missing from training data!\")\n",
        "if target_column in test.columns:\n",
        "    test = test.drop(columns=[target_column], errors=\"ignore\")\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "# Remove target from numerical columns list\n",
        "if target_column in numerical_cols:\n",
        "    numerical_cols.remove(target_column)\n",
        "\n",
        "# Remove 'uid' from feature lists\n",
        "for col_list in [numerical_cols, categorical_cols]:\n",
        "    if \"uid\" in col_list:\n",
        "        col_list.remove(\"uid\")\n",
        "\n",
        "# Handle missing values\n",
        "num_imputer = SimpleImputer(strategy=\"mean\")\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "train[numerical_cols] = num_imputer.fit_transform(train[numerical_cols])\n",
        "test[numerical_cols] = num_imputer.transform(test[numerical_cols])\n",
        "\n",
        "train[categorical_cols] = cat_imputer.fit_transform(train[categorical_cols])\n",
        "test[categorical_cols] = cat_imputer.transform(test[categorical_cols])\n",
        "\n",
        "# Convert categorical columns to numeric (Label Encoding)\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    train[col] = le.fit_transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "\n",
        "# Scale Numerical Features\n",
        "scaler = StandardScaler()\n",
        "train[numerical_cols] = scaler.fit_transform(train[numerical_cols])\n",
        "test[numerical_cols] = scaler.transform(test[numerical_cols])\n",
        "\n",
        "# Split data\n",
        "X = train.drop(columns=[target_column, \"uid\"], errors=\"ignore\")\n",
        "y = train[target_column]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define RandomForest model with optimized hyperparameters\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,      # Reduce trees for faster training\n",
        "    max_depth=15,          # Prevent overfitting\n",
        "    max_features=\"sqrt\",   # Use sqrt of features to speed up training\n",
        "    n_jobs=-1,             # Use all CPU cores\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = rf_model.predict(X_train)\n",
        "y_val_pred = rf_model.predict(X_val)\n",
        "y_test_pred = rf_model.predict(test.drop(columns=[\"uid\"], errors=\"ignore\"))\n",
        "\n",
        "# Calculate RMSE\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "print(f\"ğŸš€ Optimized Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"ğŸš€ Optimized Validation RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "# Save submission file\n",
        "submission = pd.DataFrame({\n",
        "    \"uid\": test[\"uid\"] if \"uid\" in test.columns else range(len(y_test_pred)),\n",
        "    \"output_electricity_generation\": y_test_pred\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"âœ… Submission file saved as 'submission.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfIW_APRx961",
        "outputId": "f82abbac-7854-4799-9f58-6a2d8f4519db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest - Train RMSE: 1.4712, Validation RMSE: 3.0785\n",
            "XGBoost - Train RMSE: 0.2847, Validation RMSE: 2.8129\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005636 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2383\n",
            "[LightGBM] [Info] Number of data points in the train set: 40320, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 832.300201\n",
            "LightGBM - Train RMSE: 1.7875, Validation RMSE: 3.1896\n",
            "\n",
            "âœ… Best Model: XGBoost with Validation RMSE: 2.8129\n",
            "âœ… Submission file saved as 'submission2.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv(\"/content/train (1).csv\")\n",
        "test = pd.read_csv(\"/content/test (1).csv\")\n",
        "# Identify target variable\n",
        "target_column = \"output_electricity_generation\"\n",
        "\n",
        "# Ensure target column exists in train but not in test\n",
        "if target_column not in train.columns:\n",
        "    raise ValueError(f\"Column '{target_column}' is missing from training data!\")\n",
        "if target_column in test.columns:\n",
        "    test = test.drop(columns=[target_column], errors=\"ignore\")\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "# Remove target from numerical columns list\n",
        "if target_column in numerical_cols:\n",
        "    numerical_cols.remove(target_column)\n",
        "\n",
        "# Remove 'uid' from feature lists\n",
        "for col_list in [numerical_cols, categorical_cols]:\n",
        "    if \"uid\" in col_list:\n",
        "        col_list.remove(\"uid\")\n",
        "\n",
        "# Handle missing values\n",
        "num_imputer = SimpleImputer(strategy=\"mean\")\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "train[numerical_cols] = num_imputer.fit_transform(train[numerical_cols])\n",
        "test[numerical_cols] = num_imputer.transform(test[numerical_cols])\n",
        "\n",
        "train[categorical_cols] = cat_imputer.fit_transform(train[categorical_cols])\n",
        "test[categorical_cols] = cat_imputer.transform(test[categorical_cols])\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    train[col] = le.fit_transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Split train into features (X) and target (y)\n",
        "X = train.drop(columns=[target_column, \"uid\"], errors=\"ignore\")\n",
        "y = train[target_column]\n",
        "\n",
        "# Split train into train/validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure test columns match training columns\n",
        "X_test = test.drop(columns=[\"uid\"], errors=\"ignore\")\n",
        "\n",
        "missing_cols = set(X_train.columns) - set(X_test.columns)\n",
        "extra_cols = set(X_test.columns) - set(X_train.columns)\n",
        "\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Test data is missing columns: {missing_cols}\")\n",
        "if extra_cols:\n",
        "    X_test = X_test.drop(columns=extra_cols, errors=\"ignore\")\n",
        "\n",
        "# Model Training Function\n",
        "def train_and_evaluate(model, model_name):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "    print(f\"{model_name} - Train RMSE: {train_rmse:.4f}, Validation RMSE: {val_rmse:.4f}\")\n",
        "    return val_rmse, model\n",
        "\n",
        "# Train multiple models\n",
        "models = {\n",
        "    \"RandomForest\": RandomForestRegressor(n_estimators=300, max_depth=20, random_state=42),\n",
        "    \"XGBoost\": xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=10, subsample=0.8, colsample_bytree=0.8, random_state=42),\n",
        "    \"LightGBM\": lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, max_depth=10, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
        "}\n",
        "\n",
        "best_rmse = float(\"inf\")\n",
        "best_model = None\n",
        "best_model_name = \"\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    val_rmse, trained_model = train_and_evaluate(model, name)\n",
        "    if val_rmse < best_rmse:\n",
        "        best_rmse = val_rmse\n",
        "        best_model = trained_model\n",
        "        best_model_name = name\n",
        "\n",
        "print(f\"\\nâœ… Best Model: {best_model_name} with Validation RMSE: {best_rmse:.4f}\")\n",
        "\n",
        "# Make final predictions using the best model\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "# Ensure submission file has exactly 21600 rows\n",
        "submission2 = pd.DataFrame({\n",
        "    \"uid\": test[\"uid\"] if \"uid\" in test.columns else range(len(y_test_pred)),\n",
        "    \"output_electricity_generation\": y_test_pred\n",
        "})\n",
        "assert submission2.shape[0] == 21600, f\"Error: Expected 21600 rows, but got {submission2.shape[0]}\"\n",
        "\n",
        "submission2.to_csv(\"submission2.csv\", index=False)\n",
        "print(\"âœ… Submission file saved as 'submission2.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY4XFCsaykOG",
        "outputId": "d95002e7-4b30-4f98-ce69-7cc19eb781f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-09 13:30:11,085] A new study created in memory with name: no-name-c33585c3-a701-4f39-a1f5-94fad52bab36\n",
            "[I 2025-02-09 13:30:33,445] Trial 0 finished with value: 2.3460826859974295 and parameters: {'n_estimators': 337, 'learning_rate': 0.10538995244811226, 'max_depth': 8, 'subsample': 0.6224248745853117, 'colsample_bytree': 0.984706641670051}. Best is trial 0 with value: 2.3460826859974295.\n",
            "[I 2025-02-09 13:30:45,127] Trial 1 finished with value: 2.2653929909592856 and parameters: {'n_estimators': 551, 'learning_rate': 0.1622431750093771, 'max_depth': 8, 'subsample': 0.9665068283279222, 'colsample_bytree': 0.6967476392733617}. Best is trial 1 with value: 2.2653929909592856.\n",
            "[I 2025-02-09 13:31:06,622] Trial 2 finished with value: 1.985266307322321 and parameters: {'n_estimators': 792, 'learning_rate': 0.08234996281643882, 'max_depth': 11, 'subsample': 0.7197753878674782, 'colsample_bytree': 0.6029289913015466}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:31:56,483] Trial 3 finished with value: 2.068852047983394 and parameters: {'n_estimators': 716, 'learning_rate': 0.05956734789315741, 'max_depth': 14, 'subsample': 0.8735909772394379, 'colsample_bytree': 0.6748902606711247}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:31:57,973] Trial 4 finished with value: 2.3633754438832857 and parameters: {'n_estimators': 586, 'learning_rate': 0.12244718855510425, 'max_depth': 4, 'subsample': 0.6260857919467425, 'colsample_bytree': 0.9930522134831945}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:32:01,193] Trial 5 finished with value: 2.608969035125183 and parameters: {'n_estimators': 984, 'learning_rate': 0.03793174662681293, 'max_depth': 4, 'subsample': 0.8349828293065515, 'colsample_bytree': 0.8221233065574818}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:32:30,447] Trial 6 finished with value: 2.7541300836758156 and parameters: {'n_estimators': 932, 'learning_rate': 0.10136765839086245, 'max_depth': 10, 'subsample': 0.8063436143325308, 'colsample_bytree': 0.9896181032351686}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:32:32,416] Trial 7 finished with value: 2.365979741286858 and parameters: {'n_estimators': 483, 'learning_rate': 0.053822013233563934, 'max_depth': 6, 'subsample': 0.7596423322514533, 'colsample_bytree': 0.64447659654917}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:32:36,497] Trial 8 finished with value: 2.27423352384105 and parameters: {'n_estimators': 342, 'learning_rate': 0.19988409819739988, 'max_depth': 9, 'subsample': 0.8968466008411655, 'colsample_bytree': 0.6408489625588302}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:32:50,990] Trial 9 finished with value: 2.1767406915331975 and parameters: {'n_estimators': 644, 'learning_rate': 0.04979380742947784, 'max_depth': 10, 'subsample': 0.9479746517846905, 'colsample_bytree': 0.7373503435755475}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:33:43,894] Trial 10 finished with value: 2.1572702270166855 and parameters: {'n_estimators': 794, 'learning_rate': 0.011414961244742575, 'max_depth': 14, 'subsample': 0.7164540914032186, 'colsample_bytree': 0.8427740180517317}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:34:34,146] Trial 11 finished with value: 2.117235230637623 and parameters: {'n_estimators': 786, 'learning_rate': 0.07798917190513702, 'max_depth': 14, 'subsample': 0.8760747125076032, 'colsample_bytree': 0.6192103341428763}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:35:05,866] Trial 12 finished with value: 2.0575218306363814 and parameters: {'n_estimators': 771, 'learning_rate': 0.07576264102253208, 'max_depth': 12, 'subsample': 0.7260065579762398, 'colsample_bytree': 0.7364847019101621}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:35:42,077] Trial 13 finished with value: 2.1025265147231096 and parameters: {'n_estimators': 863, 'learning_rate': 0.13372150389815862, 'max_depth': 12, 'subsample': 0.7109372463677263, 'colsample_bytree': 0.7604671620339689}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:36:19,336] Trial 14 finished with value: 2.16799824920238 and parameters: {'n_estimators': 741, 'learning_rate': 0.0820154261060671, 'max_depth': 12, 'subsample': 0.6881186171869467, 'colsample_bytree': 0.8930848262388235}. Best is trial 2 with value: 1.985266307322321.\n",
            "[I 2025-02-09 13:36:40,306] Trial 15 finished with value: 1.9192815062992183 and parameters: {'n_estimators': 861, 'learning_rate': 0.022231509653634268, 'max_depth': 11, 'subsample': 0.7654208115639898, 'colsample_bytree': 0.600963148270801}. Best is trial 15 with value: 1.9192815062992183.\n",
            "[I 2025-02-09 13:37:04,921] Trial 16 finished with value: 1.9772329709696843 and parameters: {'n_estimators': 890, 'learning_rate': 0.01308799384447756, 'max_depth': 11, 'subsample': 0.7670393128233673, 'colsample_bytree': 0.6030295046021459}. Best is trial 15 with value: 1.9192815062992183.\n",
            "[I 2025-02-09 13:37:08,566] Trial 17 finished with value: 2.4954164157543652 and parameters: {'n_estimators': 888, 'learning_rate': 0.014419676648790535, 'max_depth': 6, 'subsample': 0.7920081676471885, 'colsample_bytree': 0.6684156295214208}. Best is trial 15 with value: 1.9192815062992183.\n",
            "[I 2025-02-09 13:38:26,705] Trial 18 finished with value: 1.8827457577814055 and parameters: {'n_estimators': 975, 'learning_rate': 0.034668114523981414, 'max_depth': 15, 'subsample': 0.6734466615511067, 'colsample_bytree': 0.7003723753445754}. Best is trial 18 with value: 1.8827457577814055.\n",
            "[I 2025-02-09 13:39:42,051] Trial 19 finished with value: 1.8796489558069158 and parameters: {'n_estimators': 995, 'learning_rate': 0.02545362244830009, 'max_depth': 15, 'subsample': 0.6520848604211458, 'colsample_bytree': 0.7038703585960145}. Best is trial 19 with value: 1.8796489558069158.\n",
            "[I 2025-02-09 13:41:05,371] Trial 20 finished with value: 1.89934417294943 and parameters: {'n_estimators': 921, 'learning_rate': 0.03514991061781646, 'max_depth': 15, 'subsample': 0.6601196816510695, 'colsample_bytree': 0.7775195600442847}. Best is trial 19 with value: 1.8796489558069158.\n",
            "[I 2025-02-09 13:42:32,660] Trial 21 finished with value: 1.893455833834276 and parameters: {'n_estimators': 993, 'learning_rate': 0.03349092134095573, 'max_depth': 15, 'subsample': 0.6509970086413703, 'colsample_bytree': 0.7786868849906207}. Best is trial 19 with value: 1.8796489558069158.\n",
            "[I 2025-02-09 13:43:54,987] Trial 22 finished with value: 1.8638823038420387 and parameters: {'n_estimators': 985, 'learning_rate': 0.03681042213065991, 'max_depth': 15, 'subsample': 0.6623745913511481, 'colsample_bytree': 0.7093572523471423}. Best is trial 22 with value: 1.8638823038420387.\n",
            "[I 2025-02-09 13:45:09,108] Trial 23 finished with value: 1.9375559156569386 and parameters: {'n_estimators': 990, 'learning_rate': 0.0653200510200989, 'max_depth': 15, 'subsample': 0.6105617302463516, 'colsample_bytree': 0.7241252448926258}. Best is trial 22 with value: 1.8638823038420387.\n",
            "[I 2025-02-09 13:45:58,924] Trial 24 finished with value: 1.8737670541368026 and parameters: {'n_estimators': 943, 'learning_rate': 0.03680607810720677, 'max_depth': 13, 'subsample': 0.6703472171006696, 'colsample_bytree': 0.7069634809297458}. Best is trial 22 with value: 1.8638823038420387.\n",
            "[I 2025-02-09 13:46:42,528] Trial 25 finished with value: 1.9116507166794532 and parameters: {'n_estimators': 833, 'learning_rate': 0.04573656239824627, 'max_depth': 13, 'subsample': 0.6529599940265017, 'colsample_bytree': 0.6907202582475904}. Best is trial 22 with value: 1.8638823038420387.\n",
            "[I 2025-02-09 13:47:42,018] Trial 26 finished with value: 2.0812956794767623 and parameters: {'n_estimators': 933, 'learning_rate': 0.02850897879640958, 'max_depth': 13, 'subsample': 0.6860862919146969, 'colsample_bytree': 0.8646414268580717}. Best is trial 22 with value: 1.8638823038420387.\n",
            "[I 2025-02-09 13:48:25,237] Trial 27 finished with value: 1.9805280796620146 and parameters: {'n_estimators': 925, 'learning_rate': 0.09478597619934277, 'max_depth': 13, 'subsample': 0.6355938686233242, 'colsample_bytree': 0.6575039533199614}. Best is trial 22 with value: 1.8638823038420387.\n",
            "[I 2025-02-09 13:49:10,022] Trial 28 finished with value: 2.074958151988669 and parameters: {'n_estimators': 694, 'learning_rate': 0.06305381205381211, 'max_depth': 14, 'subsample': 0.7389281134145277, 'colsample_bytree': 0.7172305339769117}. Best is trial 22 with value: 1.8638823038420387.\n",
            "[I 2025-02-09 13:49:58,254] Trial 29 finished with value: 2.5352000875135463 and parameters: {'n_estimators': 829, 'learning_rate': 0.12246712725384237, 'max_depth': 13, 'subsample': 0.6926625230701084, 'colsample_bytree': 0.9301745504398484}. Best is trial 22 with value: 1.8638823038420387.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best XGBoost Params: {'n_estimators': 985, 'learning_rate': 0.03681042213065991, 'max_depth': 15, 'subsample': 0.6623745913511481, 'colsample_bytree': 0.7093572523471423}\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003745 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2383\n",
            "[LightGBM] [Info] Number of data points in the train set: 31185, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 920.688998\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "XGBoost RMSE: 1.8639, LightGBM RMSE: 2.3818\n",
            "Ensemble Model RMSE: 1.8832\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008758 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2383\n",
            "[LightGBM] [Info] Number of data points in the train set: 38982, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 920.908883\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "âœ… Submission file saved as 'submission5.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv(\"/content/train (1).csv\")\n",
        "test = pd.read_csv(\"/content/test (1).csv\")\n",
        "\n",
        "# Identify target variable\n",
        "target_column = \"output_electricity_generation\"\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "# Remove target and 'uid' from numerical columns\n",
        "numerical_cols = [col for col in numerical_cols if col not in [target_column, \"uid\"]]\n",
        "\n",
        "# Handle missing values\n",
        "num_imputer = SimpleImputer(strategy=\"mean\")\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "train[numerical_cols] = num_imputer.fit_transform(train[numerical_cols])\n",
        "test[numerical_cols] = num_imputer.transform(test[numerical_cols])\n",
        "\n",
        "train[categorical_cols] = cat_imputer.fit_transform(train[categorical_cols])\n",
        "test[categorical_cols] = cat_imputer.transform(test[categorical_cols])\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    train[col] = le.fit_transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Remove outliers using IQR method\n",
        "def remove_outliers(df, col):\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    return df[(df[col] >= (Q1 - 1.5 * IQR)) & (df[col] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "train = remove_outliers(train, target_column)\n",
        "\n",
        "# Split train into features (X) and target (y)\n",
        "X = train.drop(columns=[target_column, \"uid\"], errors=\"ignore\")\n",
        "y = train[target_column]\n",
        "\n",
        "# Split train into train/validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure test data matches training columns\n",
        "X_test = test.drop(columns=[\"uid\"], errors=\"ignore\")\n",
        "\n",
        "# Hyperparameter Tuning with Optuna\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1000),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBRegressor(**params, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    return np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "best_params = study.best_params\n",
        "print(f\"Best XGBoost Params: {best_params}\")\n",
        "\n",
        "# Train XGBoost with best params\n",
        "xgb_model = xgb.XGBRegressor(**best_params, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_xgb_val = xgb_model.predict(X_val)\n",
        "xgb_rmse = np.sqrt(mean_squared_error(y_val, y_xgb_val))\n",
        "\n",
        "# Train LightGBM\n",
        "lgb_model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, max_depth=10, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "y_lgb_val = lgb_model.predict(X_val)\n",
        "lgb_rmse = np.sqrt(mean_squared_error(y_val, y_lgb_val))\n",
        "\n",
        "print(f\"XGBoost RMSE: {xgb_rmse:.4f}, LightGBM RMSE: {lgb_rmse:.4f}\")\n",
        "\n",
        "# Ensemble: Average predictions of XGBoost & LightGBM\n",
        "y_val_pred_ensemble = (y_xgb_val * 0.6) + (y_lgb_val * 0.4)\n",
        "ensemble_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_ensemble))\n",
        "print(f\"Ensemble Model RMSE: {ensemble_rmse:.4f}\")\n",
        "\n",
        "# Train final models on full data\n",
        "xgb_model.fit(X, y)\n",
        "lgb_model.fit(X, y)\n",
        "\n",
        "y_test_xgb = xgb_model.predict(X_test)\n",
        "y_test_lgb = lgb_model.predict(X_test)\n",
        "\n",
        "# Ensemble Predictions\n",
        "y_test_pred = (y_test_xgb * 0.6) + (y_test_lgb * 0.4)\n",
        "\n",
        "# Save Submission\n",
        "submission5 = pd.DataFrame({\"uid\": test[\"uid\"], \"output_electricity_generation\": y_test_pred})\n",
        "submission5.to_csv(\"submission5.csv\", index=False)\n",
        "print(\"âœ… Submission file saved as 'submission5.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNDibPBE00nX",
        "outputId": "28aeeb76-077c-401e-877f-bf3b0e59b228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m383.4/383.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "G8lOSbDr09XT",
        "outputId": "c542cbf8-af43-4b09-bdcd-2c51eebc0574"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'super' object has no attribute '__sklearn_tags__'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-77ecac65441d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mstacking_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# all_estimators contains all estimators, the one to be fitted and the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# 'drop' string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_final_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_validate_estimators\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"drop\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_estimator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 raise ValueError(\n\u001b[1;32m    235\u001b[0m                     \"The estimator {} should be a {}.\".format(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mis_regressor\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_estimator_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"regressor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"regressor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_tags.py\u001b[0m in \u001b[0;36mget_tags\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"__sklearn_tags__\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0msklearn_tags_provider\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mclass_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"_more_tags\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m__sklearn_tags__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"regressor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegressorTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# Enable IterativeImputer before importing\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv(\"/content/train (1).csv\")\n",
        "test = pd.read_csv(\"/content/test (1).csv\")\n",
        "\n",
        "# Identify target variable\n",
        "target_column = \"output_electricity_generation\"\n",
        "\n",
        "# Ensure target column exists in train but NOT in test\n",
        "if target_column not in train.columns:\n",
        "    raise ValueError(f\"Column '{target_column}' is missing from training data!\")\n",
        "if target_column in test.columns:\n",
        "    test = test.drop(columns=[target_column], errors=\"ignore\")\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "# Remove target from numerical columns list\n",
        "if target_column in numerical_cols:\n",
        "    numerical_cols.remove(target_column)\n",
        "\n",
        "# Remove 'uid' from all feature lists\n",
        "for col_list in [numerical_cols, categorical_cols]:\n",
        "    if \"uid\" in col_list:\n",
        "        col_list.remove(\"uid\")\n",
        "\n",
        "# Advanced Missing Value Imputation\n",
        "num_imputer = IterativeImputer(max_iter=10, random_state=42)  # Iterative instead of KNN\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")  # Mode instead of constant\n",
        "\n",
        "train[numerical_cols] = num_imputer.fit_transform(train[numerical_cols])\n",
        "test[numerical_cols] = num_imputer.transform(test[numerical_cols])\n",
        "\n",
        "train[categorical_cols] = cat_imputer.fit_transform(train[categorical_cols])\n",
        "test[categorical_cols] = cat_imputer.transform(test[categorical_cols])\n",
        "\n",
        "# One-Hot Encode Categorical Variables\n",
        "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "train_encoded = encoder.fit_transform(train[categorical_cols])\n",
        "test_encoded = encoder.transform(test[categorical_cols])\n",
        "\n",
        "# Convert encoded arrays back to DataFrame\n",
        "train_encoded = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(categorical_cols), index=train.index)\n",
        "test_encoded = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out(categorical_cols), index=test.index)\n",
        "\n",
        "# Drop original categorical columns and add the new encoded ones\n",
        "train = train.drop(columns=categorical_cols).join(train_encoded)\n",
        "test = test.drop(columns=categorical_cols).join(test_encoded)\n",
        "\n",
        "\n",
        "# Scale Numerical Features (RobustScaler instead of StandardScaler)\n",
        "scaler = RobustScaler()\n",
        "train[numerical_cols] = scaler.fit_transform(train[numerical_cols])\n",
        "test[numerical_cols] = scaler.transform(test[numerical_cols])\n",
        "\n",
        "# Split train into features (X) and target (y)\n",
        "X = train.drop(columns=[target_column, \"uid\"], errors=\"ignore\")\n",
        "y = train[target_column]\n",
        "\n",
        "# Split train data into train and validation sets for RMSE evaluation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure 'uid' is dropped from test before prediction\n",
        "X_test = test.drop(columns=[\"uid\"], errors=\"ignore\")\n",
        "\n",
        "# Ensure test columns match training columns\n",
        "missing_cols = set(X_train.columns) - set(X_test.columns)\n",
        "extra_cols = set(X_test.columns) - set(X_train.columns)\n",
        "\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Test data is missing columns: {missing_cols}\")\n",
        "if extra_cols:\n",
        "    X_test = X_test.drop(columns=extra_cols, errors=\"ignore\")  # Drop unexpected columns\n",
        "\n",
        "# Define Base Models\n",
        "rf_model = RandomForestRegressor(n_estimators=500, max_depth=15, min_samples_split=5,\n",
        "                                 min_samples_leaf=2, max_features=\"sqrt\", random_state=42)\n",
        "\n",
        "xgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=10,\n",
        "                         colsample_bytree=0.8, subsample=0.8, random_state=42)\n",
        "\n",
        "lgbm_model = LGBMRegressor(n_estimators=500, learning_rate=0.05, max_depth=10,\n",
        "                           num_leaves=31, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
        "\n",
        "# Define Stacking Regressor\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=[(\"rf\", rf_model), (\"xgb\", xgb_model), (\"lgbm\", lgbm_model)],\n",
        "    final_estimator=GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "    passthrough=True, n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train Model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = stacking_model.predict(X_train)\n",
        "y_val_pred = stacking_model.predict(X_val)\n",
        "y_test_pred = stacking_model.predict(X_test)\n",
        "\n",
        "# Calculate RMSE\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "print(f\"ğŸš€ Optimized Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"ğŸš€ Optimized Validation RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "# Ensure submission file has exactly 21600 rows\n",
        "submission = pd.DataFrame({\n",
        "    \"uid\": test[\"uid\"] if \"uid\" in test.columns else range(len(y_test_pred)),\n",
        "    \"output_electricity_generation\": y_test_pred\n",
        "})\n",
        "assert submission.shape[0] == 21600, f\"Error: Expected 21600 rows, but got {submission.shape[0]}\"\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"âœ… Submission file saved as 'submission.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqnWRkWywd77",
        "outputId": "154b7759-599e-4688-e188-dc80cae39e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 11.58\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.experimental import enable_iterative_imputer  # Fix IterativeImputer Import\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Sample Dataset (Replace with your dataset)\n",
        "np.random.seed(42)\n",
        "data = pd.DataFrame({\n",
        "    'feature1': np.random.randn(100),\n",
        "    'feature2': np.random.rand(100) * 100,\n",
        "    'feature3': np.random.choice(['A', 'B', 'C'], 100),\n",
        "    'target': np.random.randn(100) * 10\n",
        "})\n",
        "\n",
        "# Splitting Data\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical & numerical columns\n",
        "categorical_cols = ['feature3']\n",
        "numerical_cols = ['feature1', 'feature2']\n",
        "\n",
        "# Impute Missing Values\n",
        "num_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
        "train[numerical_cols] = num_imputer.fit_transform(train[numerical_cols])\n",
        "test[numerical_cols] = num_imputer.transform(test[numerical_cols])\n",
        "\n",
        "# One-Hot Encoding Fix (Updated for sklearn 1.3+)\n",
        "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # Fix\n",
        "train_encoded = encoder.fit_transform(train[categorical_cols])\n",
        "test_encoded = encoder.transform(test[categorical_cols])\n",
        "\n",
        "# Convert Encoded Features into DataFrame\n",
        "train_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "test_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Reset Index Before Merging\n",
        "train_encoded_df.index = train.index\n",
        "test_encoded_df.index = test.index\n",
        "\n",
        "# Merge Encoded Features with Numeric Data\n",
        "train_final = pd.concat([train[numerical_cols], train_encoded_df, train['target']], axis=1)\n",
        "test_final = pd.concat([test[numerical_cols], test_encoded_df, test['target']], axis=1)\n",
        "\n",
        "# Splitting Features and Target\n",
        "X_train, y_train = train_final.drop(columns=['target']), train_final['target']\n",
        "X_test, y_test = test_final.drop(columns=['target']), test_final['target']\n",
        "\n",
        "# Define Base Models\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Define Stacking Regressor\n",
        "stacking_model = StackingRegressor(estimators=[('rf', rf), ('gb', gb)], final_estimator=RandomForestRegressor())\n",
        "\n",
        "# Fix `__sklearn_tags__` AttributeError for sklearn 1.3+\n",
        "if not hasattr(StackingRegressor, \"__sklearn_tags__\"):\n",
        "    StackingRegressor.__sklearn_tags__ = lambda self: {}\n",
        "\n",
        "# Train Model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "\n",
        "# Evaluate Model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbMHISmxxpO9",
        "outputId": "7f2b2eea-5c87-46e2-ae67-0d0f0fe7be82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv(\"/content/train (1).csv\")\n",
        "test = pd.read_csv(\"/content/test (1).csv\")\n",
        "\n",
        "# Identify target variable\n",
        "target_column = \"output_electricity_generation\"\n",
        "\n",
        "# Ensure target column exists in train but NOT in test\n",
        "if target_column not in train.columns:\n",
        "    raise ValueError(f\"Column '{target_column}' is missing from training data!\")\n",
        "if target_column in test.columns:\n",
        "    test = test.drop(columns=[target_column], errors=\"ignore\")\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "# Remove target from numerical columns list\n",
        "if target_column in numerical_cols:\n",
        "    numerical_cols.remove(target_column)\n",
        "\n",
        "# Remove 'uid' from all feature lists\n",
        "for col_list in [numerical_cols, categorical_cols]:\n",
        "    if \"uid\" in col_list:\n",
        "        col_list.remove(\"uid\")\n",
        "\n",
        "# Handle missing values\n",
        "num_imputer = IterativeImputer(max_iter=10, random_state=42)  # Best imputation method\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")  # Mode imputation\n",
        "\n",
        "train[numerical_cols] = num_imputer.fit_transform(train[numerical_cols])\n",
        "test[numerical_cols] = num_imputer.transform(test[numerical_cols])\n",
        "\n",
        "train[categorical_cols] = cat_imputer.fit_transform(train[categorical_cols])\n",
        "test[categorical_cols] = cat_imputer.transform(test[categorical_cols])\n",
        "\n",
        "# One-Hot Encode Categorical Variables\n",
        "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "train_encoded = encoder.fit_transform(train[categorical_cols])\n",
        "test_encoded = encoder.transform(test[categorical_cols])\n",
        "\n",
        "# Convert to DataFrame and merge with numerical data\n",
        "train_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "test_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "train_final = pd.concat([train[numerical_cols], train_encoded_df], axis=1)\n",
        "test_final = pd.concat([test[numerical_cols], test_encoded_df], axis=1)\n",
        "\n",
        "# Scale Numerical Features using RobustScaler (better for outliers)\n",
        "scaler = RobustScaler()\n",
        "train_final[numerical_cols] = scaler.fit_transform(train_final[numerical_cols])\n",
        "test_final[numerical_cols] = scaler.transform(test_final[numerical_cols])\n",
        "\n",
        "# Split train into features (X) and target (y)\n",
        "X = train_final\n",
        "y = train[target_column]\n",
        "\n",
        "# Train-test split for evaluation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning with RandomizedSearchCV\n",
        "param_dist = {\n",
        "    \"n_estimators\": [500, 1000, 1500],\n",
        "    \"max_depth\": [15, 20, 30],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "    \"max_features\": [\"sqrt\"]\n",
        "}\n",
        "\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    rf_model, param_distributions=param_dist, n_iter=30, cv=5, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, random_state=42\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "best_rf = search.best_estimator_\n",
        "\n",
        "# Train Gradient Boosting Model\n",
        "gb_model = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Stacking Regressor\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=[(\"rf\", best_rf), (\"gb\", gb_model)],\n",
        "    final_estimator=GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = stacking_model.predict(X_train)\n",
        "y_val_pred = stacking_model.predict(X_val)\n",
        "y_test_pred = stacking_model.predict(test_final)\n",
        "\n",
        "# Calculate RMSE\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "print(f\"ğŸš€ Optimized Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"ğŸš€ Optimized Validation RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "# Ensure submission file has exactly 21600 rows\n",
        "submission = pd.DataFrame({\n",
        "    \"uid\": test[\"uid\"] if \"uid\" in test.columns else range(len(y_test_pred)),\n",
        "    \"output_electricity_generation\": y_test_pred\n",
        "})\n",
        "assert submission.shape[0] == 21600, f\"Error: Expected 21600 rows, but got {submission.shape[0]}\"\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"âœ… Submission file saved as 'submission.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQkEamCMyCxm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}